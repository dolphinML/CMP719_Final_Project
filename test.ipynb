{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An End-to-End Transformer Model for Crowd Localization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook comprises the paper analysis named \"An End-to-End Transformer Model for Crowd Localization\" and implementation details from my perspective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata of the Paper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Field      | Value                                     |\n",
    "|------------|-------------------------------------------|\n",
    "| Title      | An end-to-end transformer model for crowd localization |\n",
    "| Author(s)  | Liang, Dingkang<br>Xu, Wei<br>Bai, Xiang  |\n",
    "| Book Title | Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part I |\n",
    "| Pages      | 38--54                                    |\n",
    "| Year       | 2022                                      |\n",
    "| Organization | Springer                                 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Aim of This Study"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liang et al. (2022) suggests that <u>predicting head positions for crowd localization</u> is a more practical and advanced task than simply counting the number of people in a crowd. The authors propose a new method called CLTR, which is an end-to-end Crowd Localization Transformer that solves this task in a regression-based paradigm. This approach treats crowd localization as a direct set prediction problem, <u>utilizing extracted features and trainable embeddings as inputs to the transformer-decoder</u>. To generate more reasonable matching results and reduce ambiguous points, the authors introduce a <u>KMO-based Hungarian matcher</u> that considers nearby context as auxiliary matching cost. The effectiveness of this proposed method is evaluated on five datasets with various data settings, and it achieves the best localization performance on the NWPU-Crowd, UCF-QNRF, and ShanghaiTech Part A datasets.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/1.PNG\" alt=\"Image\" />\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Details"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed approach by authors, \"An End-to-End Transformer Model for Crowd Localization,\" aims to <u>predict crowd instances directly without the need for additional pre-processing or post-processing steps</u>. The implementation consists of a CNN-based backbone, a transformer encoder, a transformer decoder, and a KMO-based matcher.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/2.PNG\" alt=\"Image\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "* Below are the key components and steps involved in the implementation:\n",
    "\n",
    "    * __CNN-based Backbone__: The first step is to extract feature maps from the input image. In this study, the ResNet50 architecture is utilized as the backbone network for its strong feature extraction capabilities.\n",
    "\n",
    "    * __Feature Map Flattening__: The extracted feature maps are flattened into a 1D sequence, which is then enriched with positional embedding (Fp) to provide spatial information.\n",
    "\n",
    "    * __Transformer Encoder__: The flattened sequence with positional embedding (Fp) is passed through a transformer encoder layer, resulting in encoded features (Fe). To reduce the channel dimension of the extracted feature maps, a 1x1 convolution is applied.\n",
    "\n",
    "    * __Transformer Decoder__: The transformer decoder layers take the trainable head queries (Qh) and the encoded features (Fe) as input. Through cross-attention mechanisms, the decoder layers interact with each other, generating the decoded embedding (Fd), which contains both point (person's head) and category information.\n",
    "\n",
    "    * __Point Regression and Classification Heads__: The decoded embeddings (Fd) are subsequently decoupled into point coordinates and confidence scores using a point regression head and a classification head, respectively. This enables precise localization of crowd instances and classification into specific categories.\n",
    "\n",
    "    * __KMO-based Matcher__: During the model training process, it is necessary to match the predictions with ground truth (GT) by employing a one-to-one correspondence. Unmatched predicted points are considered as belonging to the \"background\" class.\n",
    "\n",
    "    * __Data Augmentation__: To enhance the model's robustness and generalization, various data augmentation techniques are employed during training. These include random cropping, random scaling, and horizontal flipping of the training data.\n",
    "\n",
    "    * __Optimizer and Learning Rate__: The Adam optimizer with a learning rate of 1e-4 is utilized to optimize the model parameters.\n",
    "\n",
    "    * __Datasets__: The proposed model is evaluated on three benchmark datasets: UCF-QNRF, JHU-Crowd++, and NWPU-Crowd."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Keynotes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The authors point out that regression-based methods, which predict coordinates directly, are more straightforward than detection-based and map-based methods. One advantage of these methods is that they can be trained end-to-end, without the need for preprocessing steps such as creating pseudo ground truth boxes or maps. Moreover, they do not rely on complex multi-scale fusion mechanisms to produce high-quality feature maps.\n",
    "\n",
    "* The proposed method in this study is inspired from the paper \"End-to-end object detection with transformers\". This method provides accurate object detection results in a simpler and more effective way. However, the authors note that it cannot be directly applied to crowd localization due to the intrinsic limitations of the matcher. Specifically, the key component in DETR (the method used in the \"End-to-end object detection with transformers\" paper) is the L1-based Hungarian matcher, which measures the L1 distance of bounding boxes with class confidence to match the prediction-ground truth bounding box pairs, showing superior performance in object detection. However, in crowd datasets, no bounding boxes are given, and for crowd localization, L1 distance can easily lead to ambiguous matching in the point-to-point pairs. Crowd images only contain one category (heads), and the dense heads usually have similar textures, reporting close confidence, which can confuse the matcher. Therefore, the authors introduce a new k-nearest neighbors (KNN) matching objective named KMO as an auxiliary matching cost. The KMO-based Hungarian considers the context from nearby heads, which helps to reduce the ambiguous points and generate more reasonable matching results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As part of the CMP719 lecture project, the following tables have been utilized as benchmarks with the aim of achieving comparable results to those presented in the referenced paper. These tables provide an achieved results of the crowd counting performance based on the NWPU and UCF-QNRF datasets by authors.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/3.PNG\" alt=\"Image\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/4.PNG\" alt=\"Image\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "* It is also aimed that the visual results will be given for intuition for achieved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "from config import return_args, args\n",
    "torch.cuda.set_device(int(args.gpu_id[0]))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import dataset\n",
    "import math\n",
    "from utils import get_root_logger, setup_seed\n",
    "import nni\n",
    "from nni.utils import merge_parameter\n",
    "import time\n",
    "import util.misc as utils\n",
    "from utils import save_checkpoint\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter  # add tensoorboard\n",
    "\n",
    "if args.backbone == 'resnet50' or args.backbone == 'resnet101':\n",
    "    from Networks.CDETR import build_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "'''fixed random seed '''\n",
    "setup_seed(args.seed)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if args['dataset'] == 'jhu':\n",
    "        test_file = './npydata/jhu_val.npy'\n",
    "    elif args['dataset'] == 'nwpu':\n",
    "        test_file = './npydata/nwpu_val.npy'\n",
    "\n",
    "    with open(test_file, 'rb') as outfile:\n",
    "        test_list = np.load(outfile).tolist()\n",
    "\n",
    "    utils.init_distributed_mode(return_args)\n",
    "    model, criterion, postprocessors = build_model(return_args)\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "    model = nn.DataParallel(model, device_ids=[int(data) for data in list(args['gpu_id']) if data!=','])\n",
    "    path = './save_file/log_file/debug/'\n",
    "    args['save_path'] = path\n",
    "    if not os.path.exists(args['save_path']):\n",
    "        os.makedirs(path)\n",
    "    logger = get_root_logger(path + 'debug.log')\n",
    "    writer = SummaryWriter(path)\n",
    "\n",
    "    num_params = 0\n",
    "    for param in model.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(\"model params:\", num_params / 1e6)\n",
    "    logger.info(\"model params: = {:.3f}\\t\".format(num_params / 1e6))\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {'params': model.parameters(), 'lr': args['lr']},\n",
    "        ], lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    if args['local_rank'] == 0:\n",
    "        logger.info(args)\n",
    "\n",
    "    if not os.path.exists(args['save_path']):\n",
    "        os.makedirs(args['save_path'])\n",
    "\n",
    "    if args['pre']:\n",
    "        if os.path.isfile(args['pre']):\n",
    "            logger.info(\"=> loading checkpoint '{}'\".format(args['pre']))\n",
    "            checkpoint = torch.load(args['pre'])\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "            args['start_epoch'] = checkpoint['epoch']\n",
    "            args['best_pred'] = checkpoint['best_prec1']\n",
    "        else:\n",
    "            logger.info(\"=> no checkpoint found at '{}'\".format(args['pre']))\n",
    "\n",
    "    print('best result:', args['best_pred'])\n",
    "    logger.info('best result = {:.3f}'.format(args['best_pred']))\n",
    "    torch.set_num_threads(args['workers'])\n",
    "\n",
    "    if args['local_rank'] == 0:\n",
    "        logger.info('best result={:.3f}\\t start epoch={:.3f}'.format(args['best_pred'], args['start_epoch']))\n",
    "\n",
    "    test_data = test_list\n",
    "    if args['local_rank'] == 0:\n",
    "        logger.info('start training!')\n",
    "\n",
    "    eval_epoch = 0\n",
    "\n",
    "    pred_mae, pred_mse, visi = validate(test_data, model, criterion, logger, args)\n",
    "\n",
    "    writer.add_scalar('Metrcis/MAE', pred_mae, eval_epoch)\n",
    "    writer.add_scalar('Metrcis/MSE', pred_mse, eval_epoch)\n",
    "\n",
    "    # save_result\n",
    "    if args['save']:\n",
    "        is_best = pred_mae < args['best_pred']\n",
    "        args['best_pred'] = min(pred_mae, args['best_pred'])\n",
    "        save_checkpoint({\n",
    "            'arch': args['pre'],\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': args['best_pred'],\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, visi, is_best, args['save_path'])\n",
    "\n",
    "    if args['local_rank'] == 0:\n",
    "        logger.info(\n",
    "            'mae={:.3f}\\t mse={:.3f}\\t best_mae={:.3f}\\t'.format(\n",
    "                args['epochs'],\n",
    "                pred_mae, pred_mse,\n",
    "                args['best_pred']))\n",
    "\n",
    "\n",
    "def collate_wrapper(batch):\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    fname = []\n",
    "\n",
    "    for item in batch:\n",
    "\n",
    "        if return_args.train_patch:\n",
    "            fname.append(item[0])\n",
    "\n",
    "            for i in range(0, len(item[1])):\n",
    "                imgs.append(item[1][i])\n",
    "\n",
    "            for i in range(0, len(item[2])):\n",
    "                targets.append(item[2][i])\n",
    "        else:\n",
    "            fname.append(item[0])\n",
    "            imgs.append(item[1])\n",
    "            targets.append(item[2])\n",
    "\n",
    "    return fname, torch.stack(imgs, 0), targets\n",
    "\n",
    "\n",
    "def validate(Pre_data, model, criterion, logger, args):\n",
    "    if args['local_rank'] == 0:\n",
    "        logger.info('begin test')\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset.listDataset(Pre_data, args['save_path'],\n",
    "                            shuffle=False,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                            std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "                            ]),\n",
    "                            args=args, train=False),\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    mae = 0.0\n",
    "    mse = 0.0\n",
    "    visi = []\n",
    "\n",
    "    for i, (fname, img, kpoint, targets, patch_info) in enumerate(test_loader):\n",
    "\n",
    "        if len(img.shape) == 5:\n",
    "            img = img.squeeze(0)\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        if len(kpoint.shape) == 5:\n",
    "            kpoint = kpoint.squeeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img = img.cuda()\n",
    "            outputs = model(img)\n",
    "\n",
    "        out_logits, out_point = outputs['pred_logits'], outputs['pred_points']\n",
    "        prob = out_logits.sigmoid()\n",
    "        prob = prob.view(1, -1, 2)\n",
    "        out_logits = out_logits.view(1, -1, 2)\n",
    "        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1),\n",
    "                                               kpoint.shape[0] * args['num_queries'], dim=1)\n",
    "        count = 0\n",
    "        gt_count = torch.sum(kpoint).item()\n",
    "        for k in range(topk_values.shape[0]):\n",
    "            sub_count = topk_values[k, :]\n",
    "            sub_count[sub_count < args['threshold']] = 0\n",
    "            sub_count[sub_count > 0] = 1\n",
    "            sub_count = torch.sum(sub_count).item()\n",
    "            count += sub_count\n",
    "\n",
    "        mae += abs(count - gt_count)\n",
    "        mse += abs(count - gt_count) * abs(count - gt_count)\n",
    "\n",
    "        if i % 30 == 0:\n",
    "            print('{fname} Gt {gt:.2f} Pred {pred}'.format(fname=fname[0], gt=gt_count, pred=count))\n",
    "\n",
    "    mae = mae / len(test_loader)\n",
    "    mse = math.sqrt(mse / len(test_loader))\n",
    "\n",
    "    print('mae', mae, 'mse', mse)\n",
    "    return mae, mse, visi\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tuner_params = nni.get_next_parameter()\n",
    "    params = vars(merge_parameter(return_args, tuner_params))\n",
    "\n",
    "    main(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
